{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Joint Angle prediction from IMU data using deep learning.\n",
        "\n",
        "As a part of the Article: Gait Speed and Task-Specificity in Predicting Lower-Limb Kinematics: A Deep Learning Approach Using Inertial Sensors\n",
        "\n",
        "DOI: (Will be added)\n",
        "\n",
        "Author: Vaibhav R. Shah\n",
        "\n",
        "We recomand you run on google collabe."
      ],
      "metadata": {
        "id": "L6Z4I91-hbsv"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nXPhe-SPha8E"
      },
      "outputs": [],
      "source": [
        "# Need to downgrad the tensorflow because of new updates model can not be load so downgrad to 2.14.0\n",
        "\n",
        "!pip install tensorflow==2.14.0\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Loading libraries"
      ],
      "metadata": {
        "id": "98IaL4T6h0IK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import keras\n",
        "\n",
        "print(\"TensorFlow version:\", tf.__version__)\n",
        "print(\"Keras version:\", keras.__version__)"
      ],
      "metadata": {
        "id": "9dLzqEH9hs96"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import GaitLab2Go as GL2G\n",
        "import pandas as pd\n",
        "#import split\n"
      ],
      "metadata": {
        "id": "cf95nzCohwoO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "38j2BzD2r0n6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Processing data"
      ],
      "metadata": {
        "id": "v9eJQjo6h35n"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "> Download the folder using the bellow link and upload it in to your google drive.\n",
        "\n",
        "https://drive.google.com/drive/folders/1kS1hnL9cWWvhvKcsM0ocJgO_j4XArwUv?usp=sharing"
      ],
      "metadata": {
        "id": "ncvJEwCwh6TA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lab=GL2G.data_processing()\n",
        "fld='/content/drive/MyDrive/Joint-Angle-prediction-data/data'\n",
        "ext='.zoo'"
      ],
      "metadata": {
        "id": "LXlNzEeAh_gu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Converting .zoo files to .pkl files (Compatable with GaitLab2Go library)"
      ],
      "metadata": {
        "id": "6tUIqPFM_2nD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the IMU variables from lab's variables zoo and store in 'variables'\n",
        "variables = lab.variables_zoo_IMU()\n",
        "\n",
        "# Remove the last 18 variables from the list\n",
        "#variables = variables[:-18]\n",
        "\n",
        "# Find files in the specified folder 'fld' with the extension specified in 'ext'\n",
        "fl = lab.find_files(path=fld, ext=ext)\n",
        "\n",
        "# Convert files from Zoo format to dictionary format using 'variables' and store the result\n",
        "lab.zoo2dictionary(fl, variables)\n",
        "\n",
        "# Set file extension to '.pkl' for further processing\n",
        "ext = '.pkl'\n",
        "\n",
        "# Find files again in 'fld' directory, but now with the new extension '.pkl'\n",
        "fl = lab.find_files(path=fld, ext=ext)\n",
        "\n",
        "# Specify the list of subjects for processing (in this case, a single subject 'pp054')\n",
        "subject_list = ['pp054']\n",
        "\n",
        "# List of files to process (found previously with the '.pkl' extension)\n",
        "file_list = fl\n",
        "\n",
        "# Process the specified subject files using the subject and file lists\n",
        "lab.process_subject_files(subject_list, file_list)\n",
        "\n",
        "# Load the first file in the file list (pickled data) into a Pandas DataFrame 'x'\n",
        "x = pd.read_pickle(fl[0])\n",
        "\n",
        "# Extract a list of variable names from the 'data' attribute in the loaded file\n",
        "variable = list(x.data.keys())\n"
      ],
      "metadata": {
        "id": "4C6mo5F7ASjN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to process data for specific subjects and tasks, based on provided file list and variables\n",
        "def process_subject_data_task(subject_list, file_list, variable):\n",
        "    # Initialize an empty dictionary 'data' to store processed data for each variable\n",
        "    data = {}\n",
        "\n",
        "    # Initialize each variable in 'data' with an array of zeros with shape (1, 101)\n",
        "    for var in variable:\n",
        "        data[var] = np.zeros([1, 101])\n",
        "\n",
        "    # Add subject and task keys to 'data' with initial values\n",
        "    data['subject'] = np.array('test')\n",
        "    data['task'] = np.array([99])\n",
        "\n",
        "    # Loop over each subject in the subject list\n",
        "    for sub in subject_list:\n",
        "        # Loop over each file in file_list that contains the subject identifier\n",
        "        for fn in file_list[np.char.find(file_list, sub) > 0]:\n",
        "            print(fn)  # Print file name for debugging\n",
        "\n",
        "            # Extract task identifier from file name\n",
        "            Task = fn.split('/')[-1].split('_')[2]\n",
        "\n",
        "            # Determine the task type based on Task identifier and assign task_num accordingly\n",
        "            if Task == '01' or Task == '05':\n",
        "                task_num = 0\n",
        "                print('walk')\n",
        "            elif Task == '02' or Task == '04':\n",
        "                task_num = 1\n",
        "                # Uncomment below to print 'jog' if needed\n",
        "                # print('jog')\n",
        "            elif Task == '03':\n",
        "                task_num = 2\n",
        "                # Uncomment below to print 'run' if needed\n",
        "                # print('run')\n",
        "\n",
        "            # Load data from the file as a Pandas DataFrame 'x'\n",
        "            x = pd.read_pickle(fn)\n",
        "\n",
        "            # Append the subject identifier to 'data' under 'subject' key\n",
        "            data['subject'] = np.append(data['subject'], sub)\n",
        "\n",
        "            # Append the task number to 'data' under 'task' key\n",
        "            data['task'] = np.append(data['task'], task_num)\n",
        "\n",
        "            # Loop over each variable in the loaded data\n",
        "            for var in x.data.keys():\n",
        "                # Append the Ncycle data for each variable along axis 0\n",
        "                data[var] = np.append(data[var], x.Ncycle_data[var], axis=0)\n",
        "\n",
        "    # Return the processed data dictionary\n",
        "    return data\n"
      ],
      "metadata": {
        "id": "E9MlTmr4iVrf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Assign the subject list to the variable 'subject'\n",
        "subject = subject_list\n",
        "\n",
        "# Assign the file list to 'files' (previously found using lab.find_files)\n",
        "files = fl  # Replace with your actual file list if needed\n",
        "\n",
        "# Call the process_subject_data_task function with the subject list, file list, and variable list\n",
        "result_data = process_subject_data_task(subject, files, variable)\n"
      ],
      "metadata": {
        "id": "3Z0fU_v9iZ9_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def process_Rknee_data(data):\n",
        "    # Create an empty dictionary to store processed knee data\n",
        "    knee_data = {}\n",
        "\n",
        "    # Loop through keys in the input data dictionary\n",
        "    for var in data.keys():\n",
        "        # Include only specific variables related to shank, thigh, knee angles, and subject\n",
        "        if 'shankR' in var or 'thighR' in var or 'subject' in var:\n",
        "            knee_data[var] = data[var]\n",
        "\n",
        "    # Get the shape of the 'shankR_Acc_X' array (assuming it exists) to define dimensions for reshaping\n",
        "    x = knee_data['shankR_Acc_X'].shape\n",
        "\n",
        "    # Lambda function to reshape and concatenate accelerometer and gyroscope data along a new third axis\n",
        "    reshape_and_concat = lambda acc_gyr: np.concatenate(\n",
        "        [knee_data[f'{part}_{axis}'].reshape(x[0], x[1], 1)\n",
        "          for part in ['shankR', 'thighR']\n",
        "          for axis in ['Acc_X', 'Acc_Y', 'Acc_Z', 'Gyr_X', 'Gyr_Y', 'Gyr_Z']], axis=2)\n",
        "\n",
        "    # Apply the lambda function to reshape and concatenate accelerometer and gyroscope data\n",
        "    nA = map(reshape_and_concat, ['Acc'])\n",
        "    nA1 = list(nA)[0]  # Convert the map object to a list and select the first element\n",
        "\n",
        "    # Create a dictionary containing processed data for the knee\n",
        "    knee_d = {\n",
        "        'train': nA1,                  # Training data for knee based on reshaped accelerometer and gyroscope data\n",
        "        'subject': knee_data['subject'], # Subject identifier\n",
        "        'task': data['task']           # Task identifier\n",
        "    }\n",
        "\n",
        "    return knee_d  # Return the processed knee data dictionary\n",
        "\n",
        "\n",
        "def process_Rhip_data(data):\n",
        "    # Create an empty dictionary to store processed hip data\n",
        "    hip_data = {}\n",
        "\n",
        "    # Loop through keys in the input data dictionary\n",
        "    for var in data.keys():\n",
        "        # Include only specific variables related to trunk, thigh, and subject\n",
        "        if 'trunk' in var or 'thighR' in var or 'subject' in var:\n",
        "            hip_data[var] = data[var]\n",
        "\n",
        "    # Get the shape of the 'trunk_Acc_X' array (assuming it exists) to define dimensions for reshaping\n",
        "    x = hip_data['trunk_Acc_X'].shape\n",
        "\n",
        "    # Lambda function to reshape and concatenate accelerometer and gyroscope data along a new third axis\n",
        "    reshape_and_concat = lambda acc_gyr: np.concatenate(\n",
        "        [hip_data[f'{part}_{axis}'].reshape(x[0], x[1], 1)\n",
        "          for part in ['trunk', 'thighR']\n",
        "          for axis in ['Acc_X', 'Acc_Y', 'Acc_Z', 'Gyr_X', 'Gyr_Y', 'Gyr_Z']], axis=2)\n",
        "\n",
        "    # Apply the lambda function to reshape and concatenate accelerometer and gyroscope data\n",
        "    nA = map(reshape_and_concat, ['Acc'])\n",
        "    nA1 = list(nA)[0]  # Convert the map object to a list and select the first element\n",
        "\n",
        "    # Create a dictionary containing processed data for the hip\n",
        "    hip_d = {\n",
        "        'train': nA1,                 # Training data for hip based on reshaped accelerometer and gyroscope data\n",
        "        'subject': hip_data['subject'], # Subject identifier\n",
        "        'task': data['task']          # Task identifier\n",
        "    }\n",
        "\n",
        "    return hip_d  # Return the processed hip data dictionary\n",
        "\n",
        "\n",
        "def process_Rankle_data(data):\n",
        "    # Create an empty dictionary to store processed ankle data\n",
        "    ankle_data = {}\n",
        "\n",
        "    # Loop through keys in the input data dictionary\n",
        "    for var in data.keys():\n",
        "        # Include only specific variables related to shank, foot, and subject\n",
        "        if 'shankR' in var or 'footR' in var or 'subject' in var:\n",
        "            ankle_data[var] = data[var]\n",
        "\n",
        "    # Get the shape of the 'shankR_Acc_X' array (assuming it exists) to define dimensions for reshaping\n",
        "    x = ankle_data['shankR_Acc_X'].shape\n",
        "\n",
        "    # Lambda function to reshape and concatenate accelerometer and gyroscope data along a new third axis\n",
        "    reshape_and_concat = lambda acc_gyr: np.concatenate(\n",
        "        [ankle_data[f'{part}_{axis}'].reshape(x[0], x[1], 1)\n",
        "          for part in ['shankR', 'footR']\n",
        "          for axis in ['Acc_X', 'Acc_Y', 'Acc_Z', 'Gyr_X', 'Gyr_Y', 'Gyr_Z']], axis=2)\n",
        "\n",
        "    # Apply the lambda function to reshape and concatenate accelerometer and gyroscope data\n",
        "    nA = map(reshape_and_concat, ['Acc'])\n",
        "    nA1 = list(nA)[0]  # Convert the map object to a list and select the first element\n",
        "\n",
        "    # Create a dictionary containing processed data for the ankle\n",
        "    ankle_d = {\n",
        "        'train': nA1,                  # Training data for ankle based on reshaped accelerometer and gyroscope data\n",
        "        'subject': ankle_data['subject'], # Subject identifier\n",
        "        'task': data['task']           # Task identifier\n",
        "    }\n",
        "\n",
        "    return ankle_d  # Return the processed ankle data dictionary\n"
      ],
      "metadata": {
        "id": "Nfk_vE6bihZ3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Process the hip data from the result data and save it as a pickle file\n",
        "Rhip_d = process_Rhip_data(result_data)         # Process right hip data from result_data\n",
        "pd.to_pickle(Rhip_d, 'test_hip.pkl')            # Save the processed hip data to 'test_hip.pkl'\n",
        "\n",
        "# Process the knee data from the result data and save it as a pickle file\n",
        "Rknee_d = process_Rknee_data(result_data)       # Process right knee data from result_data\n",
        "pd.to_pickle(Rknee_d, 'test_knee.pkl')          # Save the processed knee data to 'test_knee.pkl'\n",
        "\n",
        "# Process the ankle data from the result data and save it as a pickle file\n",
        "Rankle_d = process_Rankle_data(result_data)     # Process right ankle data from result_data\n",
        "pd.to_pickle(Rankle_d, 'test_ankle.pkl')        # Save the processed ankle data to 'test_ankle.pkl'\n"
      ],
      "metadata": {
        "id": "nnvH_scNi0LV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Testing deep learning Models"
      ],
      "metadata": {
        "id": "EBD7p-Z8i5aV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to read and preprocess data for a given subject from a pickle file\n",
        "def read_data(subject, filename):\n",
        "    # Load data from the specified pickle file\n",
        "    data = pd.read_pickle('//content//' + filename)\n",
        "\n",
        "    # Extract features (X) from the 'train' key, excluding the first sample\n",
        "    X = data['train'][1:, :, :]\n",
        "\n",
        "    # Convert gyroscope data from degrees to radians for both sets of axis data\n",
        "    X[:, :, 3:6] = np.deg2rad(X[:, :, 3:6])\n",
        "    X[:, :, 9:12] = np.deg2rad(X[:, :, 9:12])\n",
        "\n",
        "    # Convert accelerometer data from g to m/s² for both sets of axis data\n",
        "    X[:, :, 0:3] = X[:, :, 0:3] * 9.81\n",
        "    X[:, :, 6:9] = X[:, :, 6:9] * 9.81\n",
        "\n",
        "    # Initialize an array to store computed magnitudes of accelerometer and gyroscope vectors\n",
        "    nx = np.zeros((X.shape[0], X.shape[1], 4))\n",
        "    col = 0\n",
        "\n",
        "    # Compute vector magnitudes for each accelerometer and gyroscope axis set\n",
        "    for i in [0, 3, 6, 9]:\n",
        "        nx[:, :, col] = np.sqrt(X[:, :, i]**2 + X[:, :, i+1]**2 + X[:, :, i+2]**2)\n",
        "        col += 1\n",
        "\n",
        "    # Concatenate the magnitude data to the original feature array\n",
        "    X = np.concatenate((X, nx), axis=2)\n",
        "\n",
        "    # Extract subject labels from the data, excluding the first sample\n",
        "    Subject = data['subject'][1:]\n",
        "\n",
        "    # Identify unique subjects in the dataset\n",
        "    list_sub = np.unique(Subject)\n",
        "\n",
        "    # Get the indices of rows corresponding to the specified subject\n",
        "    row = np.unique(np.where(Subject == subject)[0])\n",
        "\n",
        "    # Select data for the specified subject as both features (X) and labels (y) for testing\n",
        "    x_test_f_1, y_test_f_1, subject_test_f_1 = X[row], X[row], Subject[row]\n",
        "\n",
        "    # Store the number of samples for the subject\n",
        "    size = x_test_f_1.shape[0]\n",
        "\n",
        "    # Trim data into overlapping windows for model input\n",
        "    x_test_f_1, y_test_f_1, subject_test = lab.trimdata(\n",
        "        x_test_f_1, y_test_f_1, subject_test_f_1, window_size=40, stride=5\n",
        "    )\n",
        "\n",
        "    # Return preprocessed data, labels, and subject info, along with the sample size\n",
        "    return x_test_f_1, y_test_f_1, subject_test, np.int64(size)\n"
      ],
      "metadata": {
        "id": "RV7S6AG4i3q9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to read and preprocess ankle data for a given subject from a pickle file\n",
        "def read_data_ankle(subject, filename):\n",
        "    # Load data from the specified pickle file\n",
        "    data = pd.read_pickle('//content//' + filename)\n",
        "\n",
        "    # Extract features (X) from the 'train' key, excluding the first sample\n",
        "    X = data['train'][1:, :, :]\n",
        "\n",
        "    # Convert gyroscope data from degrees to radians for both sets of axis data\n",
        "    X[:, :, 3:6] = np.deg2rad(X[:, :, 3:6])\n",
        "    X[:, :, 9:12] = np.deg2rad(X[:, :, 9:12])\n",
        "\n",
        "    # Convert accelerometer data from g to m/s² for both sets of axis data\n",
        "    X[:, :, 0:3] = X[:, :, 0:3] * 9.81\n",
        "    X[:, :, 6:9] = X[:, :, 6:9] * 9.81\n",
        "\n",
        "    # Define a 180-degree rotation matrix around the z-axis to align orientation data\n",
        "    R_z_180 = np.array([\n",
        "        [-1, 0, 0],\n",
        "        [0, -1, 0],\n",
        "        [0, 0, 1]\n",
        "    ])\n",
        "\n",
        "    # Apply the rotation matrix to gyroscope and accelerometer data to reorient it\n",
        "    X[:, :, 9:12] = np.einsum('ij,klj->kli', R_z_180, X[:, :, 9:12])\n",
        "    X[:, :, 6:9] = np.einsum('ij,klj->kli', R_z_180, X[:, :, 6:9])\n",
        "\n",
        "    # Initialize an array to store computed magnitudes of accelerometer and gyroscope vectors\n",
        "    nx = np.zeros((X.shape[0], X.shape[1], 4))\n",
        "    col = 0\n",
        "\n",
        "    # Compute vector magnitudes for each accelerometer and gyroscope axis set\n",
        "    for i in [0, 3, 6, 9]:\n",
        "        nx[:, :, col] = np.sqrt(X[:, :, i]**2 + X[:, :, i+1]**2 + X[:, :, i+2]**2)\n",
        "        col += 1\n",
        "\n",
        "    # Concatenate the magnitude data to the original feature array\n",
        "    X = np.concatenate((X, nx), axis=2)\n",
        "\n",
        "    # Extract subject labels from the data, excluding the first sample\n",
        "    Subject = data['subject'][1:]\n",
        "\n",
        "    # Identify unique subjects in the dataset\n",
        "    list_sub = np.unique(Subject)\n",
        "\n",
        "    # Get the indices of rows corresponding to the specified subject\n",
        "    row = np.unique(np.where(Subject == subject)[0])\n",
        "\n",
        "    # Select data for the specified subject as both features (X) and labels (y) for testing\n",
        "    x_test_f_1, y_test_f_1, subject_test_f_1 = X[row], X[row], Subject[row]\n",
        "\n",
        "    # Store the number of samples for the subject\n",
        "    size = x_test_f_1.shape[0]\n",
        "\n",
        "    # Trim data into overlapping windows for model input\n",
        "    x_test_f_1, y_test_f_1, subject_test = lab.trimdata(\n",
        "        x_test_f_1, y_test_f_1, subject_test_f_1, window_size=40, stride=5\n",
        "    )\n",
        "\n",
        "    # Return preprocessed data, labels, subject info, and the sample size\n",
        "    return x_test_f_1, y_test_f_1, subject_test, np.int64(size)\n"
      ],
      "metadata": {
        "id": "rkAg9QpljAZd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from math import sqrt\n",
        "\n",
        "def normalized_rmse(x, y):\n",
        "    \"\"\"Calculates the normalized root mean squared error (NRMSE) between two arrays.\n",
        "\n",
        "    Args:\n",
        "        x: The ground truth values.\n",
        "        y: The predicted values.\n",
        "\n",
        "    Returns:\n",
        "        A tuple containing:\n",
        "            - rmse: The root mean squared error between the two arrays.\n",
        "            - nrmse: The normalized RMSE between the two arrays.\n",
        "    \"\"\"\n",
        "\n",
        "    # Calculate the range of the ground truth values.\n",
        "    y_range = np.max(x) - np.min(x)\n",
        "\n",
        "    # Calculate the root mean squared error.\n",
        "    rmse = np.sqrt(((x - y) ** 2).mean())\n",
        "\n",
        "    # Calculate the normalized RMSE.\n",
        "    nrmse = rmse / y_range\n",
        "\n",
        "    return rmse, nrmse\n"
      ],
      "metadata": {
        "id": "HHjAct1ZjNuu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def combine_predictions(data, size, window_size=40, stride=5, dim=0):\n",
        "    \"\"\"Combines predictions over sliding windows and averages them to smooth the data.\n",
        "\n",
        "    Args:\n",
        "        data (np.array): The prediction data array with shape (samples, time steps, features).\n",
        "        size (int): The number of predictions to process.\n",
        "        window_size (int): The size of each sliding window.\n",
        "        stride (int): The step size for each sliding window.\n",
        "        dim (int): The index of the feature dimension to use for averaging.\n",
        "\n",
        "    Returns:\n",
        "        np.array: An array of combined predictions with smoothed values over time steps.\n",
        "    \"\"\"\n",
        "\n",
        "    # Initialize an array to store the combined predictions\n",
        "    ndata = np.zeros([1, 100])\n",
        "\n",
        "    # Iterate over each prediction in the specified size range\n",
        "    for j in range(0, size):\n",
        "\n",
        "        # Initialize an array to map frame indices based on size\n",
        "        A = np.array([0])\n",
        "        for i in range(0, 17):\n",
        "            A = np.append(A, A[i] + size)\n",
        "\n",
        "        # Define the frame limits for the sliding window approach\n",
        "        size_1 = [0, 101]\n",
        "        start_frame = [i for i in range(0, size_1[1] - window_size, stride) if i + window_size < size_1[1]]\n",
        "        end_frame = [i + window_size for i in start_frame]\n",
        "\n",
        "        # Initialize a 1D array to accumulate the predictions across frames\n",
        "        B = np.zeros(100)\n",
        "\n",
        "        # Accumulate data within each sliding window\n",
        "        for i in range(0, len(start_frame)):\n",
        "            B[start_frame[i]:end_frame[i]] += data[A[i] + j, :, dim]\n",
        "\n",
        "        # Smoothing weights for each 5-frame segment\n",
        "        di = [1, 2, 3, 4, 5, 6, 7, 8, 8, 8, 8, 8, 8, 7, 6, 5, 4, 3, 2, 1]\n",
        "\n",
        "        # Apply the weights to each segment of 5 frames to smooth the prediction data\n",
        "        for i in range(0, len(di)):\n",
        "            B[i * 5:i * 5 + 5] /= di[i]\n",
        "\n",
        "        # Reshape the smoothed prediction for current sample and append to ndata\n",
        "        B = B.reshape(1, 100)\n",
        "        ndata = np.append(ndata, B, axis=0)\n",
        "\n",
        "    return ndata\n"
      ],
      "metadata": {
        "id": "KtdLQrggjPbe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.signal import butter, filtfilt\n",
        "\n",
        "def butter_lowpass(cutoff, fs, order):\n",
        "    \"\"\"Designs a Butterworth low-pass filter.\n",
        "\n",
        "    Args:\n",
        "        cutoff (float): The cutoff frequency of the filter.\n",
        "        fs (float): The sampling frequency of the signal.\n",
        "        order (int): The order of the filter (higher order means a sharper cutoff).\n",
        "\n",
        "    Returns:\n",
        "        tuple: Filter coefficients (b, a) for the low-pass filter.\n",
        "    \"\"\"\n",
        "    nyq = 0.5 * fs  # Nyquist frequency (half of the sampling rate)\n",
        "    normal_cutoff = cutoff / nyq  # Normalized cutoff frequency\n",
        "    b, a = butter(order, normal_cutoff, btype='lowpass')  # Filter coefficients\n",
        "    return b, a\n",
        "\n",
        "def butter_lowpass_filter(data, cutoff, fs, order):\n",
        "    \"\"\"Applies a Butterworth low-pass filter to the input data.\n",
        "\n",
        "    Args:\n",
        "        data (array-like): The input signal to be filtered.\n",
        "        cutoff (float): The cutoff frequency of the filter.\n",
        "        fs (float): The sampling frequency of the signal.\n",
        "        order (int): The order of the filter.\n",
        "\n",
        "    Returns:\n",
        "        np.array: The filtered signal.\n",
        "    \"\"\"\n",
        "    b, a = butter_lowpass(cutoff, fs, order)  # Get filter coefficients\n",
        "    y = filtfilt(b, a, data)  # Apply the filter to the data\n",
        "    return y\n",
        "\n",
        "# Filter parameters\n",
        "cutoff = 6    # Cutoff frequency in Hz\n",
        "fs = 100      # Sampling frequency in Hz\n",
        "order = 4     # Filter order\n"
      ],
      "metadata": {
        "id": "bo3jyZGOjQy_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy import interpolate\n",
        "\n",
        "def Normalized_gait(A):\n",
        "    # Generate an array 'x' with values from 0 to the number of columns in A (assumes A is 2D)\n",
        "    x = np.arange(A.shape[-1])\n",
        "\n",
        "    # Replace any NaN (Not a Number) values in array A with 0 to handle missing data\n",
        "    A = np.where(np.isnan(A) == 1, 0, A)\n",
        "\n",
        "    # Interpolate values using cubic interpolation with 101 points\n",
        "    # np.linspace creates 101 equally spaced points between the min and max of 'x'\n",
        "    # Interpolate the data along axis 0 (assuming x is the axis of interest for interpolation)\n",
        "    Y = interpolate.interp1d(x, A, kind='cubic')(np.linspace(x.min(), x.max(), 101))\n",
        "\n",
        "    # Return the interpolated values (Y)\n",
        "    return Y\n"
      ],
      "metadata": {
        "id": "hpPhE4y0jSOd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install dtaidistance\n"
      ],
      "metadata": {
        "id": "PAz4jc8QjT9k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from dtaidistance import dtw\n",
        "from dtaidistance import dtw_visualisation as dtwvis\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "\n",
        "def dtw_pred(y_pred, y_test):\n",
        "    # Initialize new arrays to store the normalized gait data (101 points)\n",
        "    y_pred_new = np.zeros([len(y_pred), 101])\n",
        "    y_test_new = np.zeros([len(y_test), 101])\n",
        "\n",
        "    # Loop through each prediction and corresponding test data\n",
        "    for i in range(len(y_pred)):\n",
        "        series1 = y_pred[i]  # Predicted series\n",
        "        series2 = y_test[i]  # Actual test series\n",
        "\n",
        "        # Compute the DTW distance and the best path\n",
        "        distance, paths = dtw.warping_paths(series1, series2)  # Calculate DTW path and distance\n",
        "        best_path = dtw.best_path(paths)  # Extract the best alignment path\n",
        "\n",
        "        # Use the best path to align the two series\n",
        "        A = series1[np.array(best_path)[:, 0]]  # Aligned predicted series\n",
        "        B = series2[np.array(best_path)[:, 1]]  # Aligned test series\n",
        "\n",
        "        # Normalize the gait data by interpolating to 101 points (this normalizes the gait)\n",
        "        y_pred_new[i, :] = Normalized_gait(A)\n",
        "        y_test_new[i, :] = Normalized_gait(B)\n",
        "\n",
        "    # Apply a low-pass filter to smooth the predictions and test data\n",
        "    y_pred = butter_lowpass_filter(y_pred_new, cutoff, fs, order)\n",
        "    y_test = butter_lowpass_filter(y_test_new, cutoff, fs, order)\n",
        "\n",
        "    # Return the filtered predicted and test data\n",
        "    return y_pred, y_test\n"
      ],
      "metadata": {
        "id": "AggMjJDLjV9U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# List of subjects which models will be used\n",
        "Subjects = ['S01', 'S02', 'S03', 'S04', 'S05', 'S06', 'S07', 'S08', 'S09', 'S10', 'S11', 'S12', 'S13', 'S14', 'S15', 'S16', 'S17', 'S18']\n",
        "\n",
        "# Lists to store RMSE values\n",
        "Ormse, Onrmse = [], []\n",
        "Brmse, Bnrmse = [], []\n",
        "\n",
        "# Dictionary to store predicted values for each subject and patient\n",
        "y_pred_final_hip = {}\n",
        "\n",
        "# Define the list of patient identifiers to iterate over\n",
        "subject_list = ['pp054']\n",
        "\n",
        "# Iterate over each patient identifier\n",
        "for pp in subject_list:\n",
        "    y_pred_final_hip[pp] = {}  # Initialize the dictionary for this patient\n",
        "\n",
        "    # Iterate over each subject (e.g., S01, S02, etc.)\n",
        "    for Sub in Subjects:\n",
        "        # Read the test data for this patient and subject\n",
        "        x_test_f_1, y_test, subject_test, size_array = read_data(pp, filename='test_hip.pkl')\n",
        "\n",
        "        # Load the trained model for the given subject\n",
        "        model_1 = tf.keras.models.load_model('/content/drive/MyDrive/Joint-Angle-prediction-data/Models/Hip/all_08_03_2024_hipR_data_model' + Sub + '.h5')\n",
        "\n",
        "        # Predict using the model\n",
        "        y_pred = model_1.predict(x_test_f_1)\n",
        "\n",
        "        # Set filter parameters for signal smoothing\n",
        "        cutoff = 6  # Cutoff frequency in Hz\n",
        "        fs = 100    # Sampling frequency in Hz\n",
        "        order = 4   # Filter order\n",
        "\n",
        "        # Select the first output channel (e.g., 0:1 means selecting the first column)\n",
        "        y_pred = y_pred[:, :, 0:1]\n",
        "\n",
        "        # Combine predictions to match the original sequence dimensions\n",
        "        y_pred = combine_predictions(y_pred, np.int64(size_array), window_size=40, stride=5, dim=0)\n",
        "\n",
        "        # Apply a low-pass filter to smooth the predictions\n",
        "        y_pred = butter_lowpass_filter(y_pred, cutoff, fs, order)\n",
        "\n",
        "        # Store the processed predictions for this subject\n",
        "        y_pred_final_hip[pp][Sub] = y_pred\n"
      ],
      "metadata": {
        "id": "uOEJLf7Njrn1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Dictionary to store average predictions for each patient\n",
        "y_pred_avg_hip = {}\n",
        "\n",
        "# Iterate over each patient identifier in subject_list\n",
        "for pp in subject_list:\n",
        "    # Calculate the average of all subject predictions for this patient across subjects\n",
        "    # This computes the mean of the predictions across the values for each subject in y_pred_final_hip[pp]\n",
        "    y_pred_avg_hip[pp] = np.mean(list(y_pred_final_hip[pp].values()), axis=0)\n",
        "\n",
        "    # Plot the transposed average prediction (excluding the first row, as [1:, :] skips the first row)\n",
        "    # The predictions are in red\n",
        "    plt.plot(np.transpose(y_pred_avg_hip[pp][1:, :]), 'Red')\n",
        "\n",
        "    # Add a title to the plot with the patient identifier (pp)\n",
        "    plt.title(pp)\n",
        "\n",
        "    # Show the plot for this patient\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "Cx5GxlDvkPJc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# List of subjects to process\n",
        "Subjects = ['S01', 'S02', 'S03', 'S04', 'S05', 'S06', 'S07', 'S08', 'S09', 'S10', 'S11', 'S12', 'S13', 'S14', 'S15', 'S16', 'S17', 'S18']\n",
        "\n",
        "# Lists to store RMSE values (although not used in this snippet)\n",
        "Ormse, Onrmse = [], []\n",
        "Brmse, Bnrmse = [], []\n",
        "\n",
        "# Dictionary to store predicted values for each patient and subject\n",
        "y_pred_final_knee = {}\n",
        "\n",
        "# Define the list of patient identifiers to iterate over\n",
        "subject_list = ['pp054']\n",
        "\n",
        "# Iterate over each patient identifier in subject_list\n",
        "for pp in subject_list:\n",
        "    y_pred_final_knee[pp] = {}  # Initialize the dictionary for this patient\n",
        "\n",
        "    # Read the test data for this patient\n",
        "    x_test_f_1, y_test, subject_test, size_array = read_data(pp, filename='test_knee.pkl')\n",
        "\n",
        "    # Iterate over each subject (e.g., S01, S02, etc.)\n",
        "    for Sub in Subjects:\n",
        "        # Load the trained model for the given subject\n",
        "        model_1 = tf.keras.models.load_model('/content/drive/MyDrive/Joint-Angle-prediction-data/Models/Knee/08_03_2024_kneeR_data_model' + Sub + '.h5')\n",
        "\n",
        "        # Predict using the model\n",
        "        y_pred = model_1.predict(x_test_f_1)\n",
        "\n",
        "        # Set filter parameters for signal smoothing\n",
        "        cutoff = 6  # Cutoff frequency in Hz\n",
        "        fs = 100    # Sampling frequency in Hz\n",
        "        order = 4   # Filter order\n",
        "\n",
        "        # Select the first output channel (e.g., 0:1 means selecting the first column)\n",
        "        y_pred = y_pred[:, :, 0:1]\n",
        "\n",
        "        # Combine predictions to match the original sequence dimensions\n",
        "        y_pred = combine_predictions(y_pred, np.int64(size_array), window_size=40, stride=5, dim=0)\n",
        "\n",
        "        # Apply a low-pass filter to smooth the predictions\n",
        "        y_pred = butter_lowpass_filter(y_pred, cutoff, fs, order)\n",
        "\n",
        "        # Store the processed predictions for this subject\n",
        "        y_pred_final_knee[pp][Sub] = y_pred\n"
      ],
      "metadata": {
        "id": "CloKbd1LjZ3W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Dictionary to store average predictions for each patient\n",
        "y_pred_avg_knee = {}\n",
        "\n",
        "# Iterate over each patient identifier in subject_list\n",
        "for pp in subject_list:\n",
        "    # Calculate the average of all subject predictions for this patient across subjects\n",
        "    # This computes the mean of the predictions across the values for each subject in y_pred_final_knee[pp]\n",
        "    y_pred_avg_knee[pp] = np.mean(list(y_pred_final_knee[pp].values()), axis=0)\n",
        "\n",
        "    # Plot the transposed average prediction (excluding the first row, as [1:, :] skips the first row)\n",
        "    # The predictions are in red\n",
        "    plt.plot(np.transpose(y_pred_avg_knee[pp][1:, :]), 'Red')\n",
        "\n",
        "    # Add a title to the plot with the patient identifier (pp)\n",
        "    plt.title(pp)\n",
        "\n",
        "    # Show the plot for this patient\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "SeZDxULukTd-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# List of subjects to process\n",
        "Subjects = ['S01', 'S02', 'S03', 'S04', 'S05', 'S06', 'S07', 'S08', 'S09', 'S10', 'S11', 'S12', 'S13', 'S14', 'S15', 'S16', 'S17', 'S18']\n",
        "\n",
        "# Lists to store RMSE values (although not used in this snippet)\n",
        "Ormse, Onrmse = [], []\n",
        "Brmse, Bnrmse = [], []\n",
        "\n",
        "# Dictionary to store predicted values for each patient and subject\n",
        "y_pred_final_ankle = {}\n",
        "\n",
        "# Define the list of patient identifiers to iterate over\n",
        "subject_list = ['pp054']\n",
        "\n",
        "# Iterate over each patient identifier in subject_list\n",
        "for pp in subject_list:\n",
        "    y_pred_final_ankle[pp] = {}  # Initialize the dictionary for this patient\n",
        "\n",
        "    # Iterate over each subject (e.g., S01, S02, etc.)\n",
        "    for Sub in Subjects:\n",
        "        # Read the test data for this patient\n",
        "        x_test_f_1, y_test, subject_test, size_array = read_data_ankle(pp, filename='test_ankle.pkl')\n",
        "\n",
        "        # Load the trained model for the given subject\n",
        "        model_1 = tf.keras.models.load_model('/content/drive/MyDrive/Joint-Angle-prediction-data/Models/Ankle/all_08_03_2024_ankleR_data_model' + Sub + '.h5')\n",
        "\n",
        "        # Predict using the model\n",
        "        y_pred = model_1.predict(x_test_f_1)\n",
        "\n",
        "        # Set filter parameters for signal smoothing\n",
        "        cutoff = 6  # Cutoff frequency in Hz\n",
        "        fs = 100    # Sampling frequency in Hz\n",
        "        order = 4   # Filter order\n",
        "\n",
        "        # Select the first output channel (e.g., 0:1 means selecting the first column)\n",
        "        y_pred = y_pred[:, :, 0:1]\n",
        "\n",
        "        # Combine predictions to match the original sequence dimensions\n",
        "        y_pred = combine_predictions(y_pred, np.int64(size_array), window_size=40, stride=5, dim=0)\n",
        "\n",
        "        # Apply a low-pass filter to smooth the predictions\n",
        "        y_pred = butter_lowpass_filter(y_pred, cutoff, fs, order)\n",
        "\n",
        "        # Store the processed predictions for this subject\n",
        "        y_pred_final_ankle[pp][Sub] = y_pred\n"
      ],
      "metadata": {
        "id": "d1s880Zmj6w3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Dictionary to store average predictions for each patient\n",
        "y_pred_avg_ankle = {}\n",
        "\n",
        "# Iterate over each patient identifier in subject_list\n",
        "for pp in subject_list:\n",
        "    # Calculate the average of all subject predictions for this patient across subjects\n",
        "    # This computes the mean of the predictions across the values for each subject in y_pred_final_ankle[pp]\n",
        "    y_pred_avg_ankle[pp] = np.mean(list(y_pred_final_ankle[pp].values()), axis=0)\n",
        "\n",
        "    # Plot the transposed average prediction (excluding the first row, as [1:, :] skips the first row)\n",
        "    # The predictions are in red\n",
        "    plt.plot(np.transpose(y_pred_avg_ankle[pp][1:, :]), 'Red')\n",
        "\n",
        "    # Add a title to the plot with the patient identifier (pp)\n",
        "    plt.title(pp)\n",
        "\n",
        "    # Show the plot for this patient\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "6hUGtJlrkYAG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Save the dictionary containing the final knee predictions to a pickle file\n",
        "# 'y_pred_knee_avg_all.pkl' will store the data for later use or sharing\n",
        "pd.to_pickle(y_pred_final_knee, 'y_pred_knee_avg_all.pkl')\n",
        "\n",
        "# Save the dictionary containing the final hip predictions to a pickle file\n",
        "# 'y_pred_hip_avg_all.pkl' will store the data for later use or sharing\n",
        "pd.to_pickle(y_pred_final_hip, 'y_pred_hip_avg_all.pkl')\n",
        "\n",
        "# Save the dictionary containing the final ankle predictions to a pickle file\n",
        "# 'y_pred_ankle_avg_all.pkl' will store the data for later use or sharing\n",
        "pd.to_pickle(y_pred_final_ankle, 'y_pred_ankle_avg_all.pkl')\n"
      ],
      "metadata": {
        "id": "DnD2p3uLkcFU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the pickled data of OMC data for participant pp054\n",
        "xdata = pd.read_pickle('/content/drive/MyDrive/Joint-Angle-prediction-data/OMC_Angle_pp054.pkl')\n"
      ],
      "metadata": {
        "id": "v5D2a4W87U2g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# List of subjects for processing\n",
        "subject_list = ['pp054']\n",
        "\n",
        "# Dictionaries to store averaged predictions for each joint (hip, knee, ankle)\n",
        "y_pred_avg_hip = {}\n",
        "y_pred_avg_knee = {}\n",
        "y_pred_avg_ankle = {}\n",
        "\n",
        "# DTW condition flag (1 to run with DTW, 0 to run without)\n",
        "dtw_condition = 0\n",
        "\n",
        "# Dictionaries to store RMSE values for different conditions\n",
        "rmse_hip_n = {'walk': {}, 'all': {}}\n",
        "rmse_knee_n = {'walk': {}, 'all': {}}\n",
        "rmse_ankle_n = {'walk': {}, 'all': {}}\n",
        "\n",
        "# Loop over each patient\n",
        "for pp in subject_list:\n",
        "    # Initialize a figure for plotting\n",
        "    fig = plt.figure(figsize=(15, 5))\n",
        "    fig.suptitle(pp)\n",
        "\n",
        "    # Subplot for Hip\n",
        "    plt.subplot(1, 3, 1)\n",
        "    y_pred_avg_hip[pp] = np.mean(list(y_pred_final_hip[pp].values()), axis=0)\n",
        "\n",
        "    # Get the minimum length for the prediction and ground truth data\n",
        "    iidx = np.min([y_pred_avg_hip[pp].shape[0] - 1, np.array(xdata[pp]['hip']).shape[0]])\n",
        "\n",
        "    if iidx > 0:\n",
        "        # Get the prediction and ground truth data for hip\n",
        "        x = y_pred_avg_hip[pp][1:iidx + 1]\n",
        "        y = xdata[pp]['hip'][0:iidx]\n",
        "\n",
        "        # Normalize the gait data\n",
        "        x = Normalized_gait(np.array(x))\n",
        "\n",
        "        # Apply DTW if condition is set to 1\n",
        "        if dtw_condition == 1:\n",
        "            x, y = dtw_pred(x, y)\n",
        "\n",
        "        # Compute RMSE for hip\n",
        "        rmse1, _ = normalized_rmse(x, y)\n",
        "        rmse1 = round(rmse1, 2)\n",
        "\n",
        "        # Store the RMSE in the dictionary\n",
        "        rmse_hip_n['all'][pp] = rmse1\n",
        "\n",
        "        # Print RMSE values for hip\n",
        "        print('hip')\n",
        "        print(pp)\n",
        "        print(rmse1)\n",
        "\n",
        "        # Update the predicted values\n",
        "        y_pred_avg_hip[pp] = x\n",
        "        xdata[pp]['hip'] = y\n",
        "\n",
        "        # Plot the ground truth and predicted hip angles\n",
        "        plt.plot(np.transpose(y), 'r')\n",
        "        plt.plot(np.transpose(x), '--k')\n",
        "\n",
        "        # Title and axis labels\n",
        "        plt.title('Hip sagittal angle')\n",
        "        plt.xlabel('Time (%)')\n",
        "        plt.ylabel('Sagittal angle (degrees)')\n",
        "        plt.yticks(np.arange(-20, 40, 10))\n",
        "\n",
        "    # Subplot for Knee\n",
        "    plt.subplot(1, 3, 2)\n",
        "    y_pred_avg_knee[pp] = np.mean(list(y_pred_final_knee[pp].values()), axis=0)\n",
        "\n",
        "    if iidx > 0:\n",
        "        # Get the prediction and ground truth data for knee\n",
        "        x = y_pred_avg_knee[pp][1:iidx + 1]\n",
        "        y = xdata[pp]['knee'][0:iidx]\n",
        "\n",
        "        # Normalize the gait data\n",
        "        x = Normalized_gait(np.array(x))\n",
        "\n",
        "        # Apply DTW if condition is set to 1\n",
        "        if dtw_condition == 1:\n",
        "            x, y = dtw_pred(x, y)\n",
        "\n",
        "        # Compute RMSE for knee\n",
        "        rmse1, _ = normalized_rmse(x, y)\n",
        "        rmse1 = round(rmse1, 2)\n",
        "\n",
        "        # Store the RMSE in the dictionary\n",
        "        rmse_knee_n['all'][pp] = rmse1\n",
        "\n",
        "        # Print RMSE values for knee\n",
        "        print('knee')\n",
        "        print(pp)\n",
        "        print(rmse1)\n",
        "\n",
        "        # Update the predicted values\n",
        "        y_pred_avg_knee[pp] = x\n",
        "        xdata[pp]['knee'] = y\n",
        "\n",
        "        # Plot the ground truth and predicted knee angles\n",
        "        plt.plot(np.transpose(y), 'r')\n",
        "        plt.plot(np.transpose(x), '--k')\n",
        "\n",
        "        # Title and axis labels\n",
        "        plt.title('Knee sagittal angle')\n",
        "        plt.xlabel('Time (%)')\n",
        "        plt.ylabel('Sagittal angle (degrees)')\n",
        "        plt.yticks(np.arange(0, 80, 10))\n",
        "\n",
        "    # Subplot for Ankle\n",
        "    plt.subplot(1, 3, 3)\n",
        "    y_pred_avg_ankle[pp] = np.mean(list(y_pred_final_ankle[pp].values()), axis=0)\n",
        "\n",
        "    if iidx > 0:\n",
        "        # Get the prediction and ground truth data for ankle\n",
        "        x = y_pred_avg_ankle[pp][1:iidx + 1]\n",
        "        y = xdata[pp]['ankle'][0:iidx]\n",
        "\n",
        "        # Normalize the gait data\n",
        "        x = Normalized_gait(np.array(x))\n",
        "\n",
        "        # Apply DTW if condition is set to 1\n",
        "        if dtw_condition == 1:\n",
        "            x, y = dtw_pred(x, y)\n",
        "\n",
        "        # Compute RMSE for ankle\n",
        "        rmse1, _ = normalized_rmse(x, y)\n",
        "        rmse1 = round(rmse1, 2)\n",
        "\n",
        "        # Store the RMSE in the dictionary\n",
        "        rmse_ankle_n['all'][pp] = rmse1\n",
        "\n",
        "        # Print RMSE values for ankle\n",
        "        print('ankle')\n",
        "        print(pp)\n",
        "        print(rmse1)\n",
        "\n",
        "        # Update the predicted values\n",
        "        y_pred_avg_ankle[pp] = x\n",
        "        xdata[pp]['ankle'] = y\n",
        "\n",
        "        # Plot the ground truth and predicted ankle angles\n",
        "        plt.plot(np.transpose(y), 'r')\n",
        "        plt.plot(np.transpose(x), '--k')\n",
        "\n",
        "        # Title and axis labels\n",
        "        plt.title('Ankle sagittal angle')\n",
        "        plt.xlabel('Time (%)')\n",
        "        plt.ylabel('Sagittal angle (degrees)')\n",
        "        plt.yticks(np.arange(-40, 10, 5))\n",
        "\n",
        "    # Save the plot as .svg and .jpeg files\n",
        "    plt.savefig('/content/' + pp + '.svg')\n",
        "    plt.savefig('/content/' + pp + '.jpeg')\n",
        "\n",
        "    # Show the plot\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "Zskws130vv4I"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}